{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6xMwJ3koO0B"
   },
   "source": [
    "![alt text](https://miro.medium.com/max/1250/1*Z-UWDCFfm9KQ_Xu9OoQadw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWZvVLrYoxMI"
   },
   "source": [
    "Logistic regression uses probabilities to distinguish inputs and thereby puts them into separate bags of output classes.\n",
    "\n",
    "Consider a case where you want to sketch a relation between your basketball shot’s accuracy and the distance you shoot from. On the whole, it’s about predicting whether you make the basket or not. \n",
    "\n",
    "Let’s suppose you’re going to predict the answer using linear regression. The relation between the win (y) and distance (x) is given by a linear equation,\n",
    " \n",
    " y = mx + c. \n",
    " \n",
    "As a prerequisite, you played for a month, jotted down all the values for x and y, and now you insert the values into the equation. This completes the training phase.\n",
    "\n",
    "Later, you want to estimate the possibility of making the shot from a specific distance. You note the value x and pass it to the trained math equation described above. It will now be a static equation, i.e.\n",
    "\n",
    " y = (trained_m)x + (trained_c). \n",
    " \n",
    "As a result, a `y (win)` value flew out of the equation. Discretizing y to predict the output,  either win or lose, isn’t a great technique. Although it technically works, it isn’t a sound approach because y isn’t a probability.\n",
    "\n",
    "What about using an activation function in the final stage to compel the output to fall into either the win class or the lose class? \n",
    "\n",
    "This indeed seems like a fix to our problem because it takes the concept of probability into consideration. This technique is what’s meant by logistic regression. Yet this isn’t the whole story, so let’s get a detailed overview of the fix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0guPGa8oxdu"
   },
   "source": [
    "**TYPES OF LOGISTIC REGRESSION**\n",
    "\n",
    "Logistic regression can be one of three types based on the output values:\n",
    "\n",
    "\n",
    "1.   Binary Logistic Regression, in which the target variable has only two possible values (e.g., pass/fail or win/lose).\n",
    "2.   Multi Logistic Regression, in which the target variable has three or more possible values that are not ordered (e.g., sweet/sour/bitter or cat/dog/fox).\n",
    "1.   Ordinal Logistic Regression, in which the outputs are ordered in some way (e.g., bad/good/better/best or low/medium/high).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCyC2NTJoxhF"
   },
   "source": [
    "# **Building Logistic Regression Using TensorFlow 2.0.**\n",
    "STEP 1: IMPORTING NECESSARY MODULES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2172,
     "status": "ok",
     "timestamp": 1635862389280,
     "user": {
      "displayName": "AM Fabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16545404540494228886"
     },
     "user_tz": -330
    },
    "id": "uq_eAqVcmwe6"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOf_Ng_BxvAL"
   },
   "source": [
    "STEP 2: LOADING THE MNIST DATA SET & SETTING UP HYPERPARAMETERS AND DATA SET PARAMETERS\n",
    "\n",
    "For the logistic regression model that we’re building, we will be using the MNIST data set. MNIST data is a collection of hand-written digits that contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 255.\n",
    "\n",
    "To import the MNIST data set to our program, we use `tensorflow.keras.datasets`. Next, we load the training data set and testing data set in the variables (x_train,y_train) and (x_test,y_test) using the `mnist.load_data()` function.\n",
    "\n",
    "Then we initialize the model parameters. `num_classes` denotes the number of outputs, which is 10 as we have digits from 0 to 9 in the data set. `num_features` defines the number of input parameters, and we store 784 since each image contains 784 pixels.\n",
    "\n",
    "`learning_rate` defines the step size the model should take to converge to a minimum loss. `training_steps` defines the number of steps the model will take to train itself completely, and `batch_size` denotes the number of samples per each batch in the training process. \n",
    "\n",
    "We use `display_step` to iterate over the training steps and print them in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1900,
     "status": "ok",
     "timestamp": 1635862391174,
     "user": {
      "displayName": "AM Fabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16545404540494228886"
     },
     "user_tz": -330
    },
    "id": "MSOdCQcZwV7Y",
    "outputId": "80d07f44-e680-44e1-9ab1-7d66b2c41705"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1635862391174,
     "user": {
      "displayName": "AM Fabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16545404540494228886"
     },
     "user_tz": -330
    },
    "id": "hcPKZfizyh5u"
   },
   "outputs": [],
   "source": [
    "# MNIST dataset parameters\n",
    "num_classes = 10                                     # 0 to 9 digits\n",
    "num_features = 784                                   # 28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1635862391175,
     "user": {
      "displayName": "AM Fabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16545404540494228886"
     },
     "user_tz": -330
    },
    "id": "4LIL-9mQyt0U"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "learning_rate = 0.01\n",
    "training_steps = 1000\n",
    "batch_size = 256\n",
    "display_step = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQMLO6Idwhxi"
   },
   "source": [
    "STEP 3: PREPARING THE MNIST DATA SET\n",
    "\n",
    "In the code below, each image will be converted to float32, normalized to [0, 1] and flattened to a 1-D array of 784 features (28*28).\n",
    "\n",
    "Since the data are images, we flatten the pixel values or features into a 1-D array of size 784 using the reshape method. We also normalize the pixel intensities to make sure their values are between 0 to 1 by dividing them with 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1635862391175,
     "user": {
      "displayName": "AM Fabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16545404540494228886"
     },
     "user_tz": -330
    },
    "id": "kQ5rezREw-Al"
   },
   "outputs": [],
   "source": [
    "# Convert to float32\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1635862391175,
     "user": {
      "displayName": "AM Fabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16545404540494228886"
     },
     "user_tz": -330
    },
    "id": "2PnWaDlLxDrO"
   },
   "outputs": [],
   "source": [
    "# Flatten images to 1-D vector of 784 features (28*28)\n",
    "x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1635862391175,
     "user": {
      "displayName": "AM Fabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16545404540494228886"
     },
     "user_tz": -330
    },
    "id": "FF-H_xsfxLjO"
   },
   "outputs": [],
   "source": [
    "# Normalize images value from [0, 255] to [0, 1].\n",
    "x_train, x_test = x_train / 255., x_test / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieAkTwqX0Gy_"
   },
   "source": [
    "STEP 4: SHUFFLING AND BATCHING THE DATA\n",
    "\n",
    "We need to shuffle and batch the data before we start the actual training to avoid the model from getting biased by the data. This will allow our data to be more random and helps our model to gain higher accuracies with the test data.\n",
    "\n",
    "With the help of `tf.data.Dataset.from_tensor_slices`, we can get the slices of an array in the form of objects. The function `shuffle(5000)` randomizes the order of the data set’s examples. \n",
    "\n",
    "Here, 5000 denotes the variable `shuffle_buffer`, which tells the model to pick a sample randomly from 1 to 5000 samples. After that, there are only 4999 samples left in the buffer, so the sample 5001 gets added to the buffer. The perfect method allows having an efficient input pipeline by making input processing operations runnable in parallel to downstream GPU operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1635862391176,
     "user": {
      "displayName": "AM Fabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16545404540494228886"
     },
     "user_tz": -330
    },
    "id": "T-Tgay1g0CsJ"
   },
   "outputs": [],
   "source": [
    "# Use tf.data API to shuffle and batch data.\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g38P_Pp90uhx"
   },
   "source": [
    "STEP 5: INITIALIZING WEIGHTS AND BIASES\n",
    "\n",
    "We now initialize the weights vector and bias vector with ones and zeros, respectively using `tf.ones` and `tf.zeros`. We use `tf.variable` to define these vectors as we will be changing the values of weights and biases during the course of training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1635862391176,
     "user": {
      "displayName": "AM Fabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16545404540494228886"
     },
     "user_tz": -330
    },
    "id": "Gu-4nTiP0pSA"
   },
   "outputs": [],
   "source": [
    "# Weight of shape [784, 10], the 28*28 image features, and a total number of classes.\n",
    "W = tf.Variable(tf.ones([num_features, num_classes]), name=\"weight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1635862391176,
     "user": {
      "displayName": "AM Fabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16545404540494228886"
     },
     "user_tz": -330
    },
    "id": "TJ8yJ3p81Ca8"
   },
   "outputs": [],
   "source": [
    "# Bias of shape [10], the total number of classes.\n",
    "b = tf.Variable(tf.zeros([num_classes]), name=\"bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sINUyRuQ1LAh"
   },
   "source": [
    "STEP 6: DEFINING LOGISTIC REGRESSION AND COST FUNCTION\n",
    "\n",
    "We define the logistic_regression function below, which converts the inputs into a probability distribution proportional to the exponents of the inputs using the `softmax` function. The softmax function, which is implemented using the function `tf.nn.softmax`, also makes sure that the sum of all the inputs equals one.\n",
    "\n",
    "In the next piece of code, we encode the outputs using the function `tf.one_hot`. We also define and compute the cross-entropy function as the loss function, which is given as \n",
    "\n",
    "cross-entropy loss = -ytrue*(log(ypred))\n",
    "\n",
    "using `tf.reduce_mean` and `tf.reduce_sum`, which are analogous to the mean and sum functions using numpy such as np.mean and np.sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1635862391177,
     "user": {
      "displayName": "AM Fabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16545404540494228886"
     },
     "user_tz": -330
    },
    "id": "WivRU3Gj1HSx"
   },
   "outputs": [],
   "source": [
    "# Logistic regression (Wx + b)\n",
    "\n",
    "def logistic_regression(x):\n",
    "  # Apply softmax to normalize the logits to a probability distribution\n",
    "  return tf.nn.softmax(tf.matmul(x, W) + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1635862391177,
     "user": {
      "displayName": "AM Fabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16545404540494228886"
     },
     "user_tz": -330
    },
    "id": "nXodCjYe1yiY"
   },
   "outputs": [],
   "source": [
    "# Cross-Entropy loss function\n",
    "\n",
    "def cross_entropy(y_pred, y_true):\n",
    "  # Encode label to a one hot vector\n",
    "  y_true = tf.one_hot(y_true, depth=num_classes)\n",
    "\n",
    "  # Clip prediction values to avoid log(0) error\n",
    "  y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
    "\n",
    "  # Compute cross-entropy\n",
    "  return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65n2ExQa2YWW"
   },
   "source": [
    "STEP 7: DEFINING OPTIMIZERS AND ACCURACY METRICS\n",
    "\n",
    "Now we define a function to choose the correct prediction. When we compute the output, it gives us the probability of the given data to fit a particular class of output. \n",
    "\n",
    "We consider the correct prediction as to the class having the highest probability. We compute this using the function `tf.argmax`. We also define the stochastic gradient descent as the optimizer from several optimizers present in TensorFlow. We do this using the function `tf.optimizers.SGD`. This function takes in the learning rate as its input which defines how fast the model should reach its minimum loss or gain the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1635862391177,
     "user": {
      "displayName": "AM Fabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16545404540494228886"
     },
     "user_tz": -330
    },
    "id": "pVdgALTO2NLJ"
   },
   "outputs": [],
   "source": [
    "# Accuracy metric\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "  # Predicted class is the index of the highest score in prediction vector (i.e. argmax)\n",
    "  correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "  return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1635862391177,
     "user": {
      "displayName": "AM Fabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16545404540494228886"
     },
     "user_tz": -330
    },
    "id": "lbOrymQW2sfz"
   },
   "outputs": [],
   "source": [
    "# Stochastic gradient descent optimizer\n",
    "optimizer = tf.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSk4csRS24v1"
   },
   "source": [
    "STEP 8: OPTIMIZATION PROCESS AND UPDATING WEIGHTS AND BIASES\n",
    "\n",
    "Now we define the `run_optimization()` method where we update the weights of our model.\n",
    "\n",
    "We calculate the predictions using the `logistic_regression(x)` method by taking the inputs and find out the loss generated by comparing the predicted value and the original value present in the data set. \n",
    "\n",
    "Next, we compute the gradients using and update the weights of the model with our stochastic gradient descent optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1635862391178,
     "user": {
      "displayName": "AM Fabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16545404540494228886"
     },
     "user_tz": -330
    },
    "id": "_Hxu9o_B2yCn"
   },
   "outputs": [],
   "source": [
    "# Optimization process \n",
    "\n",
    "def run_optimization(x, y):\n",
    "  # Wrap computation inside a GradientTape for automatic differentiation\n",
    "  with tf.GradientTape() as g:\n",
    "    pred = logistic_regression(x)\n",
    "    loss = cross_entropy(pred, y)\n",
    "\n",
    "  # Compute gradients\n",
    "  gradients = g.gradient(loss, [W, b])\n",
    "\n",
    "  # Update W and b following gradients\n",
    "  optimizer.apply_gradients(zip(gradients, [W, b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9Afdtm-3oSU"
   },
   "source": [
    "STEP 9: THE TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10343,
     "status": "ok",
     "timestamp": 1635862401514,
     "user": {
      "displayName": "AM Fabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16545404540494228886"
     },
     "user_tz": -330
    },
    "id": "-zOUPtPQ3lFc",
    "outputId": "2d71e5f5-abfd-4561-a735-c97075222ac6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 50, loss: 918.834900, accuracy: 0.730469\n",
      "step: 100, loss: 95.151543, accuracy: 0.906250\n",
      "step: 150, loss: 117.975586, accuracy: 0.871094\n",
      "step: 200, loss: 136.754883, accuracy: 0.875000\n",
      "step: 250, loss: 209.071350, accuracy: 0.816406\n",
      "step: 300, loss: 90.575401, accuracy: 0.929688\n",
      "step: 350, loss: 159.829681, accuracy: 0.843750\n",
      "step: 400, loss: 279.425171, accuracy: 0.820312\n",
      "step: 450, loss: 80.711960, accuracy: 0.929688\n",
      "step: 500, loss: 73.576675, accuracy: 0.910156\n",
      "step: 550, loss: 69.959778, accuracy: 0.906250\n",
      "step: 600, loss: 184.948959, accuracy: 0.843750\n",
      "step: 650, loss: 79.176666, accuracy: 0.925781\n",
      "step: 700, loss: 67.128937, accuracy: 0.953125\n",
      "step: 750, loss: 35.166481, accuracy: 0.941406\n",
      "step: 800, loss: 63.041100, accuracy: 0.929688\n",
      "step: 850, loss: 48.798599, accuracy: 0.941406\n",
      "step: 900, loss: 52.511875, accuracy: 0.953125\n",
      "step: 950, loss: 37.899361, accuracy: 0.953125\n",
      "step: 1000, loss: 80.842911, accuracy: 0.910156\n"
     ]
    }
   ],
   "source": [
    "# Run training for the given number of steps\n",
    "\n",
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
    "  # Run the optimization to update W and b values.\n",
    "  run_optimization(batch_x, batch_y)\n",
    "  \n",
    "  if step % display_step == 0:\n",
    "    pred = logistic_regression(batch_x)\n",
    "    loss = cross_entropy(pred, batch_y)\n",
    "    acc = accuracy(pred, batch_y)\n",
    "    print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kq7IPqot4LZ7"
   },
   "source": [
    "STEP 10: TESTING MODEL ACCURACY USING THE TEST DATA\n",
    "\n",
    "Finally, we check the model accuracy by sending the test data set into our model and compute the accuracy using the accuracy function that we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ftu_DyHY3_Nq",
    "outputId": "49a203c6-1979-4cb8-f8e4-48e3f89fd11c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.884600\n"
     ]
    }
   ],
   "source": [
    "# Test model on validation set\n",
    "pred = logistic_regression(x_test)\n",
    "print(\"Test Accuracy: %f\" % accuracy(pred, y_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "TF_Log_Reg_From_Scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
